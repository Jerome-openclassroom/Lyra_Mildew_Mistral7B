# -*- coding: utf-8 -*-
"""Lyra_botrytis_7B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xwfZZfUi2ukX_0Lq4asWDgD0rRsnONoX
"""

# Bloc 1 ‚Äî Install & v√©rification des fichiers JSONL
!pip install -q --upgrade "transformers>=4.46.0" peft datasets trl accelerate huggingface_hub

import transformers
import trl
import torch

print("transformers version :", transformers.__version__)
print("trl version          :", trl.__version__)
print("torch version        :", torch.__version__)
print("CUDA dispo:", torch.cuda.is_available())

import os, json

# üîß chemins vers les datasets
train_path = "/content/sample_data/dataset_mildiou_1500_train.jsonl"
valid_path = "/content/sample_data/dataset_mildiou_validation_149.jsonl"  # petit jeu d'√©val manuel

def check_jsonl(path, n=3):
    print(f"\nüîç V√©rification du fichier : {path}")
    if not os.path.exists(path):
        print("‚ùå Fichier introuvable.")
        return False
    try:
        with open(path, "r", encoding="utf-8") as f:
            lines = [next(f).strip() for _ in range(n)]
        print(f"‚úÖ Fichier trouv√© ({os.path.getsize(path)/1024:.1f} Ko)")
        for i, line in enumerate(lines, 1):
            data = json.loads(line)
            print(f"‚Äî Ligne {i} OK, cl√©s principales :", list(data.keys()))
        print("‚úÖ Structure JSONL valide (au moins sur les 3 premi√®res lignes).")
        return True
    except Exception as e:
        print("‚ö†Ô∏è Erreur :", e)
        return False

ok_train = check_jsonl(train_path)
ok_valid = check_jsonl(valid_path)

if ok_train and ok_valid:
    print("\nüöÄ Les deux datasets sont valides et pr√™ts √† √™tre charg√©s.")
else:
    print("\n‚ö†Ô∏è Probl√®me d√©tect√© sur au moins un des fichiers.")

# Bloc 2 ‚Äî Login Hugging Face avec TOKEN_HF_Mildew
import torch, os
from google.colab import userdata
from huggingface_hub import login

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"

print("GPU :", torch.cuda.get_device_name(0))
print("CUDA dispo:", torch.cuda.is_available())

# üîê secret Colab doit s'appeler TOKEN_HF_Mildew
hf_token = userdata.get('TOKEN_HF_Mildew')
if not hf_token:
    raise ValueError("‚ùå Le secret TOKEN_HF_Botrytis est introuvable ou vide.")
os.environ["HF_TOKEN"] = hf_token

login(token=os.environ["HF_TOKEN"])
print("üîê Connexion Hugging Face OK")

# Bloc 3 ‚Äî Chargement du JSONL & application chat template avec prompt syst√®me

import json
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer

def load_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

train_rows = load_jsonl(train_path)
valid_rows = load_jsonl(valid_path)

dataset = DatasetDict({
    "train": Dataset.from_list(train_rows),
    "validation": Dataset.from_list(valid_rows)
})

print("taille train :", len(dataset["train"]))
print("taille valid :", len(dataset["validation"]))
print("exemple brut :", dataset["train"][0])

tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

SYSTEM_PROMPT = """Tu es un agronome sp√©cialis√© dans les maladies fongiques de la vigne, en particulier le mildiou (Plasmopara viticola).
Ton r√¥le est d‚Äô√©valuer le risque d‚Äôinfection √† partir d‚Äôun ensemble r√©duit de variables, puis de formuler une recommandation agronomique courte, claire et actionnable.

Les variables fournies dans les requ√™tes sont :
- Stade_phenologique (texte) : "jeunes_feuilles", "floraison", ou "autre" (stade moins sensible).
- Humectation_continue (heures) : dur√©e ininterrompue de mouillage foliaire (pluie ou ros√©e) au cours des 24 derni√®res heures. Une valeur ‚â•6h en conditions favorables (16‚Äì22¬∞C) indique un risque d‚Äôinfection.
- Temperature_moyenne (¬∞C) : temp√©rature moyenne sur les derni√®res 24h. La zone 16‚Äì22¬∞C est optimale pour l‚Äôinfection.
- Pluie_24h (mm) : cumul de pluie sur les derni√®res 24h. ‚â•10 mm d√©clenche la lib√©ration de zoospores.
- Inoculum (0 √† 1) : pression d‚Äôinoculum, bas√©e sur le nombre d‚Äôann√©es infect√©es sur les 3 derni√®res ann√©es (0.3 = faible, 0.5 = mod√©r√©, 0.7 = √©lev√©, 0.9 = tr√®s √©lev√©).

Ta r√©ponse doit toujours comporter exactement deux √©l√©ments :
1) Un niveau de risque parmi : ¬´ faible ¬ª, ¬´ moyen ¬ª, ou ¬´ √©lev√© ¬ª.
2) Une recommandation agronomique op√©rationnelle, simple et concise (1 √† 3 phrases maximum), adapt√©e au risque et aux pratiques viticoles.

R√®gles strictes :
- Ne jamais mentionner d‚Äôautres facteurs ou produire des informations non demand√©es.
- Respecter strictement le format :
  Risque : [faible/moyen/√©lev√©]
  Recommandation : [texte concis et pragmatique]

Priorit√©s pour l‚Äô√©valuation :
- La floraison (BBCH 61‚Äì69) et les jeunes feuilles (BBCH 12‚Äì17) sont des stades critiques (majoration automatique du risque).
- Une humectation ‚â•6h + temp√©rature 16‚Äì22¬∞C + pluie ‚â•10mm = conditions id√©ales pour une infection explosive.
- Les recommandations doivent √™tre directement applicables (ex : "Traiter avec du cuivre sous 24h" ou "Surveillance renforc√©e").
- Ne pas inclure de d√©tails superflus ou de justifications.

Exemple de sortie :
Risque : √©lev√©
Recommandation : Appliquer un traitement √† base de cuivre ou un fongicide syst√©mique autoris√© sous 24h, en ciblant les inflorescences et jeunes feuilles. Renouveler apr√®s une nouvelle pluie.
"""

def to_chat_text(example):
    # On suppose que example["messages"] contient d√©j√† user + assistant,
    # sans message syst√®me. On le pr√©fixe par le SYSTEM_PROMPT.
    chat_messages = [{"role": "system", "content": SYSTEM_PROMPT}] + example["messages"]
    txt = tokenizer.apply_chat_template(
        chat_messages,
        tokenize=False,
        add_generation_prompt=False,
    )
    return {"text": txt}

dataset = dataset.map(to_chat_text)

print("aper√ßu text :", dataset["train"][0]["text"][:500])

print("transformers version :", transformers.__version__)
print("trl version          :", trl.__version__)
print("torch version        :", torch.__version__)
print("CUDA dispo:", torch.cuda.is_available())

# Bloc 4 ‚Äî Chargement du mod√®le de base & configuration LoRA

from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="auto",   # accelerate r√©partit sur le GPU (et CPU si besoin)
)

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Bloc 5 ‚Äî SFTTrainer + hyperparam√®tres Mildew
from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./lyra_Mildew_LoRA",   # dossier de travail (checkpoints interm√©diaires)
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.0,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    args=training_args,
)

trainer.train()

# Bloc 6 ‚Äî Sauvegarde des adapters dans ./lyra_Mildew_adapter

output_dir = "./lyra_Mildew_adapter"

trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("Adapters sauvegard√©s dans", output_dir)
!ls -R {output_dir}

### üì§ Bloc 7 ‚Äî Push du mod√®le sur Hugging Face


from huggingface_hub import create_repo

# ‚¨áÔ∏è username HF
hf_repo_id = "jeromex1/lyra_Mildew_mistral7B_LoRA"

print("Repo cible :", hf_repo_id)

# Cr√©e le repo si besoin
create_repo(hf_repo_id, exist_ok=True)

# Push du mod√®le (adapters PEFT) et du tokenizer
trainer.model.push_to_hub(hf_repo_id)
tokenizer.push_to_hub(hf_repo_id)

print(f"‚úÖ Mod√®le et tokenizer pouss√©s sur : https://huggingface.co/{hf_repo_id}")

# Bloc 8 ‚Äî Inf√©rences de test sur quelques situations Mildiou
import torch
model.eval()

# Prompt syst√®me pour le mildiou (√† d√©finir en haut de ton script)
SYSTEM_PROMPT = """
Tu es un agronome sp√©cialis√© dans le mildiou de la vigne (Plasmopara viticola).
Ton r√¥le est d'√©valuer le risque d'infection √† partir des param√®tres suivants, puis de formuler une recommandation agronomique concise.

Format attendu :
Risque : [faible/moyen/√©lev√©]
Recommandation : [1-3 phrases max, actionnable et sp√©cifique]
"""

def format_user(stade, humectation, temperature, pluie, inoculum):
    return (
        f"Stade_phenologique: {stade}\n"
        f"Humectation_continue: {humectation}\n"
        f"Temperature_moyenne: {temperature}\n"
        f"Pluie_24h: {pluie}\n"
        f"Inoculum: {inoculum}\n\n"
        "√âvalue le risque de mildiou et propose une recommandation agronomique."
    )

def infer_case(stade, humectation, temperature, pluie, inoculum, label=""):
    user_content = format_user(stade, humectation, temperature, pluie, inoculum)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,  # Sortie stable et reproductible
        )
    # D√©codage de la r√©ponse (sans le prompt)
    generated = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True,
    ).strip()
    print("=" * 80)
    if label:
        print(f"üß™ Cas de test : {label}")
    print("Input :")
    print(user_content)
    print("\nR√©ponse du mod√®le :")
    print(generated)
    print()

# --- Cas de test : Risque FAIBLE ---
# Cas 1 : Conditions d√©favorables (temp√©rature √©lev√©e, pas de pluie)
infer_case(
    stade="autre",
    humectation=3,
    temperature=25,
    pluie=0,
    inoculum=0.2,
    label="Faible 1 ‚Äî Temp√©rature trop √©lev√©e, pas de pluie, inoculum faible",
)

# Cas 2 : Humectation insuffisante
infer_case(
    stade="jeunes_feuilles",
    humectation=4,
    temperature=18,
    pluie=2,
    inoculum=0.3,
    label="Faible 2 ‚Äî Humectation <6h, pluie faible, stade peu sensible",
)

# Cas 3 : Stade non critique + conditions limites
infer_case(
    stade="autre",
    humectation=5,
    temperature=15,
    pluie=1,
    inoculum=0.1,
    label="Faible 3 ‚Äî Temp√©rature sous-optimale, humectation courte",
)

# Cas 4 : Inoculum tr√®s faible malgr√© humectation mod√©r√©e
infer_case(
    stade="autre",
    humectation=6,
    temperature=20,
    pluie=0,
    inoculum=0.1,
    label="Faible 4 ‚Äî Humectation seuil mais pluie absente et inoculum tr√®s faible",
)

# --- Cas de test : Risque MOYEN ---
# Cas 5 : Jeunes feuilles + humectation mod√©r√©e
infer_case(
    stade="jeunes_feuilles",
    humectation=7,
    temperature=19,
    pluie=5,
    inoculum=0.5,
    label="Moyen 1 ‚Äî Jeunes feuilles sensibles, humectation proche du seuil",
)

# Cas 6 : Floraison mais conditions limites
infer_case(
    stade="floraison",
    humectation=6,
    temperature=17,
    pluie=4,
    inoculum=0.4,
    label="Moyen 2 ‚Äî Floraison mais humectation/pluie limites",
)

# Cas 7 : Humectation longue mais temp√©rature sous-optimale
infer_case(
    stade="jeunes_feuilles",
    humectation=9,
    temperature=15,
    pluie=6,
    inoculum=0.6,
    label="Moyen 3 ‚Äî Humectation prolong√©e mais temp√©rature fra√Æche",
)

# Cas 8 : Pluie mod√©r√©e + inoculum interm√©diaire
infer_case(
    stade="autre",
    humectation=8,
    temperature=20,
    pluie=7,
    inoculum=0.5,
    label="Moyen 4 ‚Äî Pluie mod√©r√©e, inoculum moyen, stade non critique",
)

# --- Cas de test : Risque √âLEV√â ---
# Cas 9 : Floraison + conditions optimales
infer_case(
    stade="floraison",
    humectation=12,
    temperature=20,
    pluie=10,
    inoculum=0.7,
    label="√âlev√© 1 ‚Äî Floraison + humectation/pluie/temp√©rature optimales",
)

# Cas 10 : Jeunes feuilles + inoculum √©lev√©
infer_case(
    stade="jeunes_feuilles",
    humectation=14,
    temperature=19,
    pluie=12,
    inoculum=0.8,
    label="√âlev√© 2 ‚Äî Jeunes feuilles, forte pression d'inoculum, conditions id√©ales",
)

# Cas 11 : Humectation extr√™me + pluie abondante
infer_case(
    stade="floraison",
    humectation=18,
    temperature=21,
    pluie=15,
    inoculum=0.6,
    label="√âlev√© 3 ‚Äî Humectation tr√®s longue + pluie abondante en floraison",
)

# Cas 12 : Tous les facteurs d√©favorables
infer_case(
    stade="floraison",
    humectation=20,
    temperature=22,
    pluie=20,
    inoculum=0.9,
    label="√âlev√© 4 ‚Äî Tous les feux au rouge (floraison, conditions extr√™mes)",
)

#########################################
# entrainement compl√©mentaire si besoin #
#########################################

# Bloc MD2 ‚Äî Run de correction 1 epoch sur le mini-dataset "high risk zone"

from datasets import Dataset
from transformers import TrainingArguments
from trl import SFTTrainer

# 1) Chargement du mini-dataset JSONL

middle_path = "/content/sample_data/Mildew_dataset_middle_30.jsonl"

def load_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

middle_rows = load_jsonl(middle_path)
middle_dataset = Dataset.from_list(middle_rows)

print("Taille mini-dataset middle zone :", len(middle_dataset))

# 2) Conversion en texte avec le m√™me chat template que le reste
middle_dataset = middle_dataset.map(to_chat_text)
print("Exemple text middle :", middle_dataset[0]["text"][:400])

# 3) Nouveaux TrainingArguments pour une micro-correction
correction_args = TrainingArguments(
    output_dir="./lyra_Mildew_LoRA_middle_fix",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,          # un peu plus doux pour une correction fine
    fp16=True,
    logging_steps=1,
    save_strategy="no",          # pas besoin de checkpoints multiples
    report_to="none",
)

correction_trainer = SFTTrainer(
    model=model,                     # mod√®le LoRA d√©j√† entra√Æn√©
    processing_class=tokenizer,
    train_dataset=middle_dataset,
    args=correction_args,
)

correction_trainer.train()

# 4) Sauvegarde des adapters corrig√©s dans un nouveau dossier
output_dir_fix = "./lyra_Mildew_adapter_middle_fix"
correction_trainer.model.save_pretrained(output_dir_fix)
tokenizer.save_pretrained(output_dir_fix)

print("‚úÖ Adapters corrig√©s sauvegard√©s dans :", output_dir_fix)
!ls -R {output_dir_fix}

# remplacement des adapters
from huggingface_hub import HfApi, upload_folder

hf_repo_id = "jeromex1/lyra_Mildew_mistral7B_LoRA"
adapter_dir_fix = "./lyra_Mildew_adapter_middle_fix"

api = HfApi()

upload_folder(
    repo_id=hf_repo_id,
    folder_path=adapter_dir_fix,
    commit_message="Update with middle-zone calibrated LoRA adapters",
)
print("‚úÖ Adapters corrig√©s pouss√©s sur Hugging Face :", hf_repo_id)